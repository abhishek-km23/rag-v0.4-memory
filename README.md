# RAG v0.4 â€” Memory, Metadata & Embedding Cache

This repository implements **v0.4** of a Retrieval-Augmented Generation (RAG) system, extending a
multi-PDF, citation-backed pipeline with **session-level memory**, **deterministic metadata**, and
**embedding reuse**.

The focus of this version is **architectural correctness**, not UI or APIs.

---

## ğŸš€ Features

### âœ… Multi-PDF RAG with Citations
- Ingest multiple PDFs
- Chunk-level retrieval
- Answers include explicit citations

### âœ… Session-Level Conversational Memory
- Maintains contextual continuity across questions
- Enables follow-ups like *â€œexplain it simplerâ€* or *â€œwhat about the previous conceptâ€*
- Memory is **prompt-level**, **session-scoped**, and **non-persistent**

### âœ… Deterministic Metadata
Each chunk contains:
- `source` â€” originating document
- `page` â€” page number
- `chunk_id` â€” stable identifier (`source__page__index`)

This ensures traceability and prepares the system for advanced filtering and evaluation.

### âœ… Embedding Cache (In-Memory)
- Each chunk generates a stable `embedding_key` (SHA-256 hash of content)
- Embeddings are reused within a run if content is unchanged
- Prevents redundant embedding computation

---

## ğŸ§± Architecture Overview

PDFs
â†“
Chunking (chunk_id + embedding_key)
â†“
FAISS Vector Store (persistent)
â†“
Retriever
â†“
LLM
â†‘
Session Memory (prompt-level)



Key design principle:
> **Retrieval finds facts. Memory preserves understanding.**

---

## ğŸ“‚ Project Structure
```
    rag-v0.4-memory/
    â”œâ”€â”€ main_ingest.py # PDF ingestion & indexing
    â”œâ”€â”€ main_qa.py # Interactive Q/A loop
    â”œâ”€â”€ core/
    â”‚ â”œâ”€â”€ chunker.py # Chunking + deterministic metadata + embedding_key
    â”‚ â”œâ”€â”€ vector_store.py # FAISS store + in-memory embedding cache
    â”‚ â”œâ”€â”€ qa.py # Answer generation with citations + memory injection
    â”‚ â”œâ”€â”€ memory.py # Session-level conversational memory (v0.4)
    â”‚ â”œâ”€â”€ retriever.py # Retriever wrapper
    â”‚ â”œâ”€â”€ embeddings.py # Embedding model loader
    â”‚ â”œâ”€â”€ citations.py # Citation formatting logic
    â”‚ â””â”€â”€ init.py
    â””â”€â”€ vector_store/ # Persistent FAISS index
```
---

## ğŸ–¼ï¸ System Flow (Pictorial View)
```mermaid
flowchart TD

    A[Your PDF]

    A --> B

    B[Chunking]
    B --> C

    C[Embedding Cache]
    C --> D

    D[FAISS Vector Store]

    D --> E

    E[Retriever]
    E --> F

    F[Context Builder]

    H[Session Memory]

    F --> G
    H --> G

    G[Prompt to LLM]

    G --> I

    I[LLM Ollama]

    I --> J

    J[Answer with Citations]

    J --> H

```
---

## ğŸ§  How to **read this diagram**
- **Top â†’ bottom** = data flow
- **Left loop (H â†’ G)** = conversational memory
- **Bottom loop (J â†’ H)** = memory update after each answer
- Retrieval path is **unchanged** by memory

This now renders **100% correctly on GitHub**.

---

## â–¶ï¸ How to Run

### 1. Activate virtual environment
```bash
.\.venv\Scripts\activate
```

### 2. Ingest PDFs

python main_ingest.py

### 3. Start Q/A session

python main_qa.py

- Ask multiple related questions in one session to observe memory behavior.

### ğŸ”¬ What This Version Does Not Do (By Design)

    âŒ No FastAPI / web server
    âŒ No LangGraph
    âŒ No persistent memory
    âŒ No disk-based embedding cache
    âŒ No evaluation benchmarks

These are planned for later versions.

### ğŸ¯ Motivation

This project is built to demonstrate clean RAG system evolution, with each version introducing
one architectural concept at a time. The code is intended to be:

- Diffable
- Explainable
- Interview-ready
