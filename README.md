# RAG v0.4 ‚Äî Memory, Metadata & Embedding Cache

This repository implements **v0.4** of a Retrieval-Augmented Generation (RAG) system, extending a
multi-PDF, citation-backed pipeline with **session-level memory**, **deterministic metadata**, and
**embedding reuse**.

The focus of this version is **architectural correctness**, not UI or APIs.

---

## üöÄ Features

### ‚úÖ Multi-PDF RAG with Citations
- Ingest multiple PDFs
- Chunk-level retrieval
- Answers include explicit citations

### ‚úÖ Session-Level Conversational Memory
- Maintains contextual continuity across questions
- Enables follow-ups like *‚Äúexplain it simpler‚Äù* or *‚Äúwhat about the previous concept‚Äù*
- Memory is **prompt-level**, **session-scoped**, and **non-persistent**

### ‚úÖ Deterministic Metadata
Each chunk contains:
- `source` ‚Äî originating document
- `page` ‚Äî page number
- `chunk_id` ‚Äî stable identifier (`source__page__index`)

This ensures traceability and prepares the system for advanced filtering and evaluation.

### ‚úÖ Embedding Cache (In-Memory)
- Each chunk generates a stable `embedding_key` (SHA-256 hash of content)
- Embeddings are reused within a run if content is unchanged
- Prevents redundant embedding computation

---

## üß± Architecture Overview

PDFs
‚Üì
Chunking (chunk_id + embedding_key)
‚Üì
FAISS Vector Store (persistent)
‚Üì
Retriever
‚Üì
LLM
‚Üë
Session Memory (prompt-level)



Key design principle:
> **Retrieval finds facts. Memory preserves understanding.**

---

## üìÇ Project Structure
```
    rag-v0.4-memory/
    ‚îú‚îÄ‚îÄ main_ingest.py # PDF ingestion & indexing
    ‚îú‚îÄ‚îÄ main_qa.py # Interactive Q/A loop
    ‚îú‚îÄ‚îÄ core/
    ‚îÇ ‚îú‚îÄ‚îÄ chunker.py # Chunking + deterministic metadata + embedding_key
    ‚îÇ ‚îú‚îÄ‚îÄ vector_store.py # FAISS store + in-memory embedding cache
    ‚îÇ ‚îú‚îÄ‚îÄ qa.py # Answer generation with citations + memory injection
    ‚îÇ ‚îú‚îÄ‚îÄ memory.py # Session-level conversational memory (v0.4)
    ‚îÇ ‚îú‚îÄ‚îÄ retriever.py # Retriever wrapper
    ‚îÇ ‚îú‚îÄ‚îÄ embeddings.py # Embedding model loader
    ‚îÇ ‚îú‚îÄ‚îÄ citations.py # Citation formatting logic
    ‚îÇ ‚îî‚îÄ‚îÄ init.py
    ‚îî‚îÄ‚îÄ vector_store/ # Persistent FAISS index
```
---

## üñºÔ∏è System Flow (Pictorial View)
```mermaid
flowchart TD

    A[Your PDF]

    A --> B

    B[Chunking]
    B --> C

    C[Embedding Cache]
    C --> D

    D[FAISS Vector Store]

    D --> E

    E[Retriever]
    E --> F

    F[Context Builder]

    H[Session Memory]

    F --> G
    H --> G

    G[Prompt to LLM]

    G --> I

    I[LLM Ollama]

    I --> J

    J[Answer with Citations]

    J --> H

```
---
## üîç How this maps EXACTLY to your diagram

| Your ASCII Box | Mermaid Box |
|---------------|-------------|
| Your PDF | `Your PDF` |
| Chunking (split, chunk_id, hash) | `Chunking` |
| Embedding Cache | `Embedding Cache` |
| FAISS Vector Store (disk) | `FAISS Vector Store` |
| Retriever | `Retriever` |
| Context Builder | `Context Builder` |
| SESSION MEMORY | `Session Memory` |
| PROMPT to LLM | `Prompt to LLM` |
| LLM (Ollama) | `LLM Ollama` |
| Answer + Citations | `Answer with Citations` |

The **loops and order are identical**.  
Only **text inside boxes** is simplified for Mermaid.

---


## ‚ñ∂Ô∏è How to Run

### 1. Activate virtual environment
```bash
.\.venv\Scripts\activate
```

### 2. Ingest PDFs

python main_ingest.py

### 3. Start Q/A session

python main_qa.py

- Ask multiple related questions in one session to observe memory behavior.

### üî¨ What This Version Does Not Do (By Design)

    ‚ùå No FastAPI / web server
    ‚ùå No LangGraph
    ‚ùå No persistent memory
    ‚ùå No disk-based embedding cache
    ‚ùå No evaluation benchmarks

These are planned for later versions.

### üéØ Motivation

This project is built to demonstrate clean RAG system evolution, with each version introducing
one architectural concept at a time. The code is intended to be:

- Diffable
- Explainable
- Interview-ready
